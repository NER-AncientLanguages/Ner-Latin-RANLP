{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latin-BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 12 11:17:28 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3090         Off| 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 30%   27C    P8               25W / 350W|      0MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce RTX 3090         Off| 00000000:41:00.0 Off |                  N/A |\r\n",
      "| 30%   26C    P8               30W / 350W|      0MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce RTX 3090         Off| 00000000:81:00.0 Off |                  N/A |\r\n",
      "| 30%   26C    P8               22W / 350W|      0MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce RTX 3090         Off| 00000000:C1:00.0 Off |                  N/A |\r\n",
      "| 30%   25C    P8               30W / 350W|      0MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor2tensor\n",
      "  Using cached tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.4.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.98)\n",
      "Requirement already satisfied: dopamine-rl in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (3.0.1)\n",
      "Requirement already satisfied: flask in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (2.3.2)\n",
      "Requirement already satisfied: future in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.18.3)\n",
      "Requirement already satisfied: gevent in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (22.10.2)\n",
      "Requirement already satisfied: gin-config in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (2.87.0)\n",
      "Requirement already satisfied: gunicorn in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (20.1.0)\n",
      "Requirement already satisfied: gym in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.26.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (3.8.0)\n",
      "Requirement already satisfied: kfac in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.2.0)\n",
      "Requirement already satisfied: mesh-tensorflow in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.1.21)\n",
      "Requirement already satisfied: numpy in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.23.5)\n",
      "Requirement already satisfied: oauth2client in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (4.1.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (4.7.0.72)\n",
      "Requirement already satisfied: Pillow in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (9.5.0)\n",
      "Requirement already satisfied: pypng in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.20220715.0)\n",
      "Requirement already satisfied: requests in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (2.31.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.10.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.12)\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.20.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: tensorflow-gan in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.7.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (0.7.0)\n",
      "Requirement already satisfied: tf-slim in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor) (4.65.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-probability==0.7.0->tensor2tensor) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=0.6.1 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-probability==0.7.0->tensor2tensor) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gym->tensor2tensor) (0.0.8)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor) (2.3.4)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor) (1.6.2)\n",
      "Requirement already satisfied: zope.event in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor) (4.6)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor) (6.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from gevent->tensor2tensor) (65.5.0)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor) (2.0.2)\n",
      "Requirement already satisfied: cffi>=1.12.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor) (1.15.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor) (2.19.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor) (0.1.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor) (2.11.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor) (4.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client->tensor2tensor) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client->tensor2tensor) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client->tensor2tensor) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->tensor2tensor) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->tensor2tensor) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->tensor2tensor) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->tensor2tensor) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->tensor2tensor) (1.3.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-addons->tensor2tensor) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-addons->tensor2tensor) (23.1)\n",
      "Requirement already satisfied: array-record in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (0.2.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (0.1.8)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (1.3.0)\n",
      "Requirement already satisfied: promise in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (4.23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (5.9.5)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (1.13.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (2.3.0)\n",
      "Requirement already satisfied: toml in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (0.10.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->tensor2tensor) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-gan->tensor2tensor) (0.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->tensor2tensor) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.12.2->gevent->tensor2tensor) (2.21)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tensor2tensor) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tensor2tensor) (4.5.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tensor2tensor) (3.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client->tensor2tensor) (1.59.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->tensor2tensor) (5.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->tensor2tensor) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u0161477\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Jinja2>=3.1.2->flask->tensor2tensor) (2.1.2)\n",
      "Installing collected packages: tensorflow-datasets, tensor2tensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\u0161477\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow_datasets\\\\datasets\\\\open_images_challenge2019_detection\\\\open_images_challenge2019_detection_dataset_builder.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\u0161477\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install tensorflow\n",
    "!pip install tensor2tensor\n",
    "# !pip install cltk\n",
    "# !pip install seqeval\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./data/Latin_NER_train.csv', index_col=0)\n",
    "test = pd.read_csv('./data/Latin_NER_test.csv', index_col=0)\n",
    "val = pd.read_csv('./data/Latin_NER_eval.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "O         82696\n",
       "B-PERS     2706\n",
       "B-GRP      1271\n",
       "B-LOC       839\n",
       "I-PERS      618\n",
       "I-LOC        31\n",
       "I-GRP         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label2idx = {'O': 0, \n",
    " 'B-PERS': 1, \n",
    " 'I-PERS': 2, \n",
    " 'B-LOC': 3, \n",
    " 'I-LOC': 4, \n",
    " 'B-GRP': 5, \n",
    " 'I-GRP': 6}\n",
    "\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "\n",
    "# train['tag'] = train['tag'].apply(lambda x:label2idx[x])\n",
    "\n",
    "# for row in train['tag']:\n",
    "#   assert type(row) == int\n",
    "\n",
    "# val['tag'] = val['tag'].apply(lambda x: label2idx[x])\n",
    "\n",
    "# for row in val['tag']:\n",
    "#   assert type(row) == int\n",
    "\n",
    "# test['tag'] = test['tag'].apply(lambda x: label2idx[x])\n",
    "\n",
    "# for row in test['tag']:\n",
    "#   assert type(row) == int\n",
    "\n",
    "# def get_dataset(df, sentence_col='sentence', token_col=\"word\", label_col=\"tag\"):\n",
    "#   for_hf = []\n",
    "  \n",
    "#   df_GROUPED = df.groupby(sentence_col)\n",
    "\n",
    "#   for group_name in df_GROUPED.groups:\n",
    "\n",
    "#     group = df_GROUPED.get_group(group_name)\n",
    "\n",
    "#     tokens = group[token_col].values.tolist()\n",
    "#     tags = group[label_col].values.tolist()\n",
    "\n",
    "\n",
    "#     dct = {'id': group_name,'tokens': tokens, 'tags': tags}\n",
    "\n",
    "#     for_hf.append(\n",
    "#       json.dumps(dct, ensure_ascii=False))\n",
    "\n",
    "#   return for_hf\n",
    "\n",
    "# TRAIN_json = get_dataset(train)\n",
    "\n",
    "# # print(TRAIN_json)\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.makedirs(\"data/DBG_json\", exist_ok=True)\n",
    "# with open(\"data/DBG_json/train.json\", \"w+\") as f:\n",
    "#     f.write(\"\\n\".join(TRAIN_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_json = get_dataset(test)\n",
    "\n",
    "# with open(\"data/DBG_json/test.json\", \"w+\") as f:\n",
    "#     f.write(\"\\n\".join(TEST_json))\n",
    "\n",
    "# DEV_json = get_dataset(val)\n",
    "\n",
    "# with open(\"data/DBG_json/validation.json\", \"w+\") as f:\n",
    "#     f.write(\"\\n\".join(DEV_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install tensorflow\n",
    "# !pip install tensor2tensor\n",
    "# !pip install cltk\n",
    "# !pip install seqeval\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0161477\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (C:/Users/u0161477/.cache/huggingface/datasets/json/Latin_NER_json-ef066b2c1b4704e0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 56.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('data/Latin_NER_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nessecary cltk models\n",
    "# !pip install cltk\n",
    "# import cltk\n",
    "# from cltk.data.fetch import FetchCorpus\n",
    "# corpus_downloader = FetchCorpus(language='lat')\n",
    "# corpus_downloader.import_corpus('lat_models_cltk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensor2tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# install tensor to tensor encoder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# !pip install tensor2tensor --user\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# !pip install tensorflow\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[39m# from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensor2tensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_generators\u001b[39;00m \u001b[39mimport\u001b[39;00m text_encoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensor2tensor'"
     ]
    }
   ],
   "source": [
    "# install tensor to tensor encoder\n",
    "# !pip install tensor2tensor --user\n",
    "# !pip install tensorflow\n",
    "\n",
    "# from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "# from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\n",
    "from tensor2tensor.data_generators import text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied this class and function from the latin-BERT repo\n",
    "#Wouters code\n",
    "#made some adjustments\n",
    "\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "class LatinTokenizer():\n",
    "\tdef __init__(self, encoder):\n",
    "\t\tself.vocab={}\n",
    "\t\tself.reverseVocab={}\n",
    "\t\tself.encoder=encoder\n",
    "\n",
    "\t\tself.vocab[\"[PAD]\"]=0\n",
    "\t\tself.vocab[\"[UNK]\"]=1\n",
    "\t\tself.vocab[\"[CLS]\"]=2\n",
    "\t\tself.vocab[\"[SEP]\"]=3\n",
    "\t\tself.vocab[\"[MASK]\"]=4\n",
    "\t\tself.model_max_length=256\n",
    "\t\tself.is_fast=False\n",
    "\n",
    "\n",
    "\t\tself.cls_token_id = self.vocab[\"[CLS]\"]\n",
    "\t\tself.pad_token_id = self.vocab[\"[PAD]\"]\n",
    "\t\tself.sep_token_id = self.vocab[\"[SEP]\"]\n",
    "        \n",
    "\t\tfor key in self.encoder._subtoken_string_to_id:\n",
    "\t\t\tself.vocab[key]=self.encoder._subtoken_string_to_id[key]+5\n",
    "\t\t\tself.reverseVocab[self.encoder._subtoken_string_to_id[key]+5]=key\n",
    "\n",
    "\n",
    "\tdef convert_tokens_to_ids(self, tokens):\n",
    "\t\twp_tokens=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tif token == \"[PAD]\":\n",
    "\t\t\t\twp_tokens.append(0)\n",
    "\t\t\telif token == \"[UNK]\":\n",
    "\t\t\t\twp_tokens.append(1)\n",
    "\t\t\telif token == \"[CLS]\":\n",
    "\t\t\t\twp_tokens.append(2)\n",
    "\t\t\telif token == \"[SEP]\":\n",
    "\t\t\t\twp_tokens.append(3)\n",
    "\t\t\telif token == \"[MASK]\":\n",
    "\t\t\t\twp_tokens.append(4)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\twp_tokens.append(self.vocab[token])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\n",
    "\tdef tokenize(self, text, split_on_tokens=True):\n",
    "\t\tif split_on_tokens:\n",
    "\t\t\ttokens = [token.lower() if token not in [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] else token for token in text]\n",
    "\t\telse: \n",
    "\t\t\ttokens = text.split()\n",
    "\n",
    "\t\twp_tokens=[] #word-piece tokens\n",
    "\n",
    "\t\tfor token in tokens:\n",
    "\t\t\t# print(token)\n",
    "\n",
    "\t\t\tif token in {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}:\n",
    "\t\t\t\twp_tokens.append(token)\n",
    "\t\t\telse:\n",
    "\n",
    "\t\t\t\twp_toks=self.encoder.encode(token)\n",
    "\n",
    "\t\t\t\tfor wp in wp_toks:\n",
    "\t\t\t\t\twp_tokens.append(self.reverseVocab[wp+5])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\t\n",
    "\tdef calculate_attention_masks(self, wp_tokens):\n",
    "\t\tattention_masks = []\n",
    "\t\t\n",
    "\t\tfor token in wp_tokens:\n",
    "\t\t\tif token == self.pad_token_id:\n",
    "\t\t\t\tattention_masks.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tattention_masks.append(1)\n",
    "\t\t\t\t\n",
    "\t\treturn attention_masks\n",
    "\t\n",
    "\tdef pad(self, features, padding=True, max_length=256, pad_to_multiple_of=\"\", return_tensors=True):\n",
    "\t\t# TODO\n",
    "\t\tbatch_outputs = {}\n",
    "\t\t\n",
    "\t\tfor i in range(len(features)):\n",
    "\t\t\tfor key, value in features[i].items():\n",
    "\t\n",
    "\t\t\t\tif key in batch_outputs:\n",
    "\t\t\t\t\tbatch_outputs[key].append(value)\n",
    "\t\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbatch_outputs[key] = [value]\n",
    "\n",
    "\t\tfor k, v in batch_outputs.items():\n",
    "\t\t\tbatch_outputs[k] = torch.tensor([x for x in v])\n",
    "\n",
    "\t\treturn BatchEncoding(batch_outputs)\n",
    "\t\n",
    "\tdef pad_max_length_and_add_specials_tokens_also(self, tokens, wp_tokens):\n",
    "\n",
    "\t\tMAX_LENGTH = 256\n",
    "\t\twp_tokens.insert(0, self.cls_token_id)\n",
    "\t\ttokens.insert(0, '[CLS]')\n",
    "\t\twp_tokens.append(self.sep_token_id)\n",
    "\t\ttokens.append('[SEP]')\n",
    "\t\t\n",
    "\t\tif len(wp_tokens) > 256:\n",
    "\t\t\twp_tokens = wp_tokens[:256]\n",
    "\t\t\ttokens = tokens[:256]\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\twhile len(wp_tokens) < 256:\n",
    "\t\t\t\twp_tokens.append(self.pad_token_id)\n",
    "\t\t\t\ttokens.append('[PAD]')\n",
    "\n",
    "\t\treturn tokens, wp_tokens\n",
    "\t\n",
    "\tdef pad_max_length_and_add_specials(self, wp_tokens):\n",
    "\n",
    "\t\tMAX_LENGTH = 256\n",
    "\t\twp_tokens.insert(0, self.cls_token_id)\n",
    "\t\twp_tokens.append(self.sep_token_id)\n",
    "\t\t\n",
    "\t\tif len(wp_tokens) > 256:\n",
    "\t\t\twp_tokens = wp_tokens[:256]\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\twhile len(wp_tokens) < 256:\n",
    "\t\t\t\twp_tokens.append(self.pad_token_id)\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\t\n",
    "\tdef decode_to_string(self, input_ids):\n",
    "\t\ttokens = [self.reverseVocab[x] for x in input_ids if x > 4]\n",
    "\t\treturn \"\".join(tokens).replace('_', ' ')\n",
    "\n",
    "\tdef save_pretrained(self, output_dir):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def convert_to_toks(sents):\n",
    "\n",
    "# \tsent_tokenizer = SentenceTokenizer()\n",
    "# \tword_tokenizer = WordTokenizer()\n",
    "\n",
    "# \tall_sents=[]\n",
    "\n",
    "# \tfor data in sents:\n",
    "# \t\ttext=data.lower()\n",
    "\n",
    "# \t\tsents=sent_tokenizer.tokenize(text)\n",
    "# \t\tfor sent in sents:\n",
    "# \t\t\ttokens=word_tokenizer.tokenize(sent)\n",
    "# \t\t\tfilt_toks=[]\n",
    "# \t\t\tfilt_toks.append(\"[CLS]\")\n",
    "# \t\t\tfor tok in tokens:\n",
    "# \t\t\t\tif tok != \"\":\n",
    "# \t\t\t\t\tfilt_toks.append(tok)\n",
    "# \t\t\tfilt_toks.append(\"[SEP]\")\n",
    "\n",
    "# \t\t\tall_sents.append(filt_toks)\n",
    "\n",
    "# \treturn all_sents\n",
    "\n",
    "# def df_to_toks(df, sent_column=\"sentence_ids\", word_column=\"words\"):\n",
    "\n",
    "# \tall_sents = []\n",
    "\n",
    "# \tgrouped = df.groupby(sent_column)\n",
    "\t\n",
    "# \tfor sent in grouped.groups:\n",
    "# \t\tsent_df = grouped.get_group(sent)\n",
    "\t\t\n",
    "# \t\ttokens = sent_df[word_column].values.tolist()\n",
    "\t\t\n",
    "# \t\tfilt_toks=[]\n",
    "\t\t\n",
    "# \t\tfilt_toks.append(\"[CLS]\")\n",
    "# \t\tfor tok in tokens:\n",
    "# \t\t\tif tok != \"\":\n",
    "# \t\t\t\tfilt_toks.append(tok)\n",
    "# \t\tfilt_toks.append(\"[SEP]\")\n",
    "\n",
    "# \t\tall_sents.append(filt_toks)\n",
    "\n",
    "# \treturn all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ut_', 'vero_', 'ex_', 'litteris_', 'ad_', 'senatum_', 'referre', 'tur_', ',_', 'impetra', 'ri_', 'non_', 'potuit_', '._', 'hi_', 'omnes_', 'lingua_', 'institutis_', ',_', 'legibus_', 'inter_', 'se_', 'differunt_', '._', '\"_', 'tu_', 'sole', 'bas_', 'nuga', 's_', 'esse_', 'aliquid_', 'meas_', 'putare_', ',_', '\"_', 'ut_', 'obit', 'er_', 'emo', 'llia', 'm_', 'catull', 'um_', 'conter', 'rane', 'um_', 'meum_', '(_', 'agnosci', 's_', 'et_', 'hoc_', 'castr', 'ense_', 'verbum_', ')_', 'enim_', ',_', 'ut_', 'scis_', ',_', 'permutat', 'is_', 'prioribus_', 'syllab', 'is_', 'duri', 'uscul', 'um_', 'se_', 'fecit_', 'quam_', 'volebat_', 'existima', 'ri_', ',_', 'a_', 'vera', 'nio', 'lis_', 'suis_', 'et_', 'fabul', 'lis_', '._']\n"
     ]
    }
   ],
   "source": [
    "#load the tokenizer\n",
    "tokenizer = LatinTokenizer(text_encoder.SubwordTextEncoder('../latin-bert/models/subword_tokenizer_latin/latin.subword.encoder'))\n",
    "\n",
    "test_sentence = train.groupby('sentence').get_group(1)['word'].values.tolist()\n",
    "\n",
    "tokens = tokenizer.tokenize(test_sentence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "#the output is unusual for huggingface, but it's in the vocab file this way so I leave it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "\t\n",
    "\tpretokenized_samples = all_samples_per_split[\"tokens\"]\n",
    "\ttokenized_samples = tokenizer.tokenize(pretokenized_samples) #create wordpiece tokens\n",
    "\ttoken_ids = tokenizer.convert_tokens_to_ids(tokenized_samples) #to ids\n",
    "    #path both the tokens and the the token_ids, as the tokenids are on subwords\n",
    "\tpadded_tokenized_samples, padded_token_ids = tokenizer.pad_max_length_and_add_specials_tokens_also(tokenized_samples, token_ids) #pad and add special tokens\n",
    "\n",
    "\tall_samples_per_split['input_ids'] = padded_token_ids\n",
    "    \n",
    "\tall_samples_per_split['attention_mask'] = tokenizer.calculate_attention_masks(padded_token_ids)\n",
    "\tall_samples_per_split['wp_tokens'] = tokenized_samples\n",
    "\tall_samples_per_split['extra'] = padded_tokenized_samples\n",
    "\n",
    "\t#original\n",
    "\torig_labels = all_samples_per_split['tags']\n",
    "\n",
    "\n",
    "\t# logic to adjust labels, \n",
    "\tadjusted_labels = []\n",
    "\tlabel_idx = 0\n",
    "\t# print(len(pretokenized_samples))\n",
    "\n",
    "\tfor token in padded_tokenized_samples:\n",
    "\t\ttry:\n",
    "            #The tokenizer always treats punctuation as a separate token\n",
    "            #in most cases, this is not a problem as the punctuation is also seperately labeled in GWannotation, \n",
    "            #but there are a few exceptions\n",
    "            #next statement catches those\n",
    "            \n",
    "\t\t\tif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "\t\t\t\tadjusted_labels.append(-100)\n",
    "                \n",
    "\t\t\telif re.match(r'\\w+[\\.\\,]', pretokenized_samples[label_idx]):\n",
    "\t\t\t\tif token != '._' and token != ',_':\n",
    "\t\t\t\t\tadjusted_labels.append(orig_labels[label_idx])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tadjusted_labels.append(orig_labels[label_idx])\n",
    "\t\t\t\t\tlabel_idx += 1\n",
    "\t\t\n",
    "\t\t\telif token.endswith('_'):\n",
    "\t\t\t\tadjusted_labels.append(orig_labels[label_idx])\n",
    "\t\t\t\tlabel_idx += 1\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tadjusted_labels.append(orig_labels[label_idx])\n",
    "\t\texcept IndexError:\n",
    "\t\t\ttry :\n",
    "\t\t\t\tif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "\t\t\t\t\tadjusted_labels.append(-100)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tadjusted_labels.append(orig_labels[label_idx])\n",
    "\t\t\texcept IndexError:\n",
    "\t\t\t\tprint('HERE')\n",
    "\t\t\t\tprint(pretokenized_samples[:label_idx-1])\n",
    "\t\t\t\tprint(orig_labels[:label_idx-1])\n",
    "\t\t\t\tprint(token)\n",
    "\t\t\t\tprint(list(zip(padded_tokenized_samples, adjusted_labels)))\n",
    "\n",
    "\n",
    "\tall_samples_per_split['labels'] = adjusted_labels\n",
    "\n",
    "\ttry:\n",
    "\t\tassert len(adjusted_labels) == len(padded_tokenized_samples) == 256\n",
    "\texcept AssertionError:\n",
    "\t\tprint(all_samples_per_split)\n",
    "\n",
    "\n",
    "\treturn all_samples_per_split\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_adjust_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_289625/3171414001.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [idx2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [idx2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install protobuf==3.20.* --user\n",
    "# model = AutoModelForTokenClassification.from_pretrained('../latin-bert/models')\n",
    "model = AutoModelForTokenClassification.from_pretrained('Herodotos_trained_lat_BERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_bert_output\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_clear_list(temp, fixed, item):\n",
    "    temp.append(item)\n",
    "    fixed.append(int(np.mean(temp)))\n",
    "    temp.clear()\n",
    "\n",
    "\n",
    "def aggregate_ents(original_tokens, wp_tokens, preds, labels):\n",
    "    #THE FUNCTION WORKS\n",
    "    #Aggregates subword\n",
    "    try:\n",
    "        assert len(wp_tokens) == len(preds) and len(wp_tokens) == len(labels)\n",
    "    except AssertionError:\n",
    "        print('lenght tokens, predictions and labels are not equal')\n",
    "        print(wp_tokens)\n",
    "        \n",
    "    fixed_preds = []\n",
    "    fixed_labels = []\n",
    "    \n",
    "    temp_label = []\n",
    "    temp_pred = []\n",
    "    \n",
    "    for i in range(len(wp_tokens)-1):\n",
    "        if (wp_tokens[i+1] == '._') and (wp_tokens[i] != '._'):\n",
    "            \n",
    "            #check if word in orig_tokens is a \\w+[,.] token (M. or H,)\n",
    "            \n",
    "            #if the original token IS one of the special cases, treat current token as not a comp\n",
    "            if re.match(r'\\w+\\.', original_tokens[len(fixed_preds)]):\n",
    "                temp_label.append(labels[i])\n",
    "                temp_pred.append(preds[i])\n",
    "            \n",
    "            else:\n",
    "                extend_clear_list(temp_label, fixed_labels, labels[i])\n",
    "                extend_clear_list(temp_pred, fixed_preds, preds[i])\n",
    "                \n",
    "        elif wp_tokens[i+1] == ',_' and (wp_tokens[i] != ',_'):\n",
    "            \n",
    "            #check if word in orig_tokens is a \\w+[,.] token (M. or H,)\n",
    "            \n",
    "            #if the original token IS one of the special cases, treat current token as not a comp\n",
    "            if re.match(r'\\w+\\,', original_tokens[len(fixed_preds)]):\n",
    "                temp_label.append(labels[i])\n",
    "                temp_pred.append(preds[i])\n",
    "            \n",
    "            else:\n",
    "                extend_clear_list(temp_label, fixed_labels, labels[i])\n",
    "                extend_clear_list(temp_pred, fixed_preds, preds[i])\n",
    "            \n",
    "    \n",
    "        elif wp_tokens[i].endswith('_') and len(temp_label) == 0:\n",
    "            fixed_preds.append(preds[i])\n",
    "            fixed_labels.append(labels[i])\n",
    "            \n",
    "        elif wp_tokens[i].endswith('_'):\n",
    "            extend_clear_list(temp_label, fixed_labels, labels[i])\n",
    "            extend_clear_list(temp_pred, fixed_preds, preds[i])\n",
    "\n",
    "        else:\n",
    "            temp_label.append(labels[i])\n",
    "            temp_pred.append(preds[i])\n",
    "            \n",
    "            \n",
    "    fixed_preds.append(preds[len(wp_tokens)-1])\n",
    "    fixed_labels.append(labels[len(wp_tokens)-1])\n",
    "    \n",
    "            \n",
    "    try:        \n",
    "        assert len(original_tokens) == len(fixed_preds) and len(original_tokens) == len(fixed_labels)\n",
    "    except AssertionError:\n",
    "        original_tokens = original_tokens[:len(fixed_preds)]\n",
    "        try:\n",
    "            assert len(original_tokens) == len(fixed_preds) and len(original_tokens) == len(fixed_labels)\n",
    "        except AssertionError:\n",
    "            print('lenght of original tokens, aggregated predictions and labels are not equal')\n",
    "            print(f'''originals = {original_tokens} \\n \n",
    "                  tokenized = {wp_tokens}\\n\n",
    "                  predictions = {preds}\\n\n",
    "                  labels = {labels}\\n\n",
    "                  fixed_preds = {fixed_preds}\n",
    "                  fixed_labels = {fixed_labels}''')\n",
    "    \n",
    "    return original_tokens, fixed_preds, fixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, extra, tags, wp_tokens. If id, tokens, extra, tags, wp_tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 972\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]\n",
    "    \n",
    "true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]\n",
    "\n",
    "tokens = tokenized_dataset['validation']['extra']\n",
    "orig_tokens = tokenized_dataset['validation']['tokens']\n",
    "ids = tokenized_dataset['validation']['id']\n",
    "\n",
    "dct = {\n",
    "    'orig_tokens_all': [],\n",
    "    'agg_predictions_all': [],\n",
    "    'agg_labels_all': [],\n",
    "    'agg_ent_predictions_all': [],\n",
    "    'agg_ent_labels_all': [],\n",
    "    'all_ids': []\n",
    "}\n",
    "\n",
    "major_l = list(zip(ids, orig_tokens, tokens, true_predictions, true_labels))\n",
    "\n",
    "for idd, original_tokens, wp_tokens, preds, labels in major_l:\n",
    "    try:\n",
    "        wp_tokens = [token for token in wp_tokens if token not in ['[CLS]', '[PAD]', '[SEP]']] \n",
    "        orig_tokens, fixed_preds, fixed_labels = aggregate_ents(original_tokens, wp_tokens, preds, labels)\n",
    "        dct['orig_tokens_all'].append(orig_tokens)\n",
    "        dct['agg_predictions_all'].append(fixed_preds)\n",
    "        dct['agg_labels_all'].append(fixed_labels)\n",
    "        dct['agg_ent_predictions_all'].append([idx2label[pred] for pred in fixed_preds])\n",
    "        dct['agg_ent_labels_all'].append([idx2label[label] for label in fixed_labels])\n",
    "        dct['all_ids'].append([idd] * len(fixed_labels))\n",
    "    except AssertionError:\n",
    "        print('\\nOh no!\\n')\n",
    "        print(original_tokens)\n",
    "        print(wp_tokens)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.91      0.82      0.86       211\n",
      "         LOC       0.77      0.82      0.79       127\n",
      "        PERS       0.85      0.87      0.86       475\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       813\n",
      "   macro avg       0.84      0.84      0.84       813\n",
      "weighted avg       0.85      0.85      0.85       813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(dct['agg_ent_labels_all'], dct['agg_ent_predictions_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnested_dct = {\n",
    "    key: sum(value, []) for key, value in dct.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13976\n",
      "13976\n",
      "13976\n",
      "13976\n",
      "13976\n",
      "13976\n"
     ]
    }
   ],
   "source": [
    "for key, value in unnested_dct.items():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "972\n",
      "972\n",
      "972\n",
      "972\n",
      "972\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aderam',\n",
       " 'Bacticis',\n",
       " 'mecumque',\n",
       " 'Lucceius',\n",
       " 'Albinus',\n",
       " ',',\n",
       " 'in',\n",
       " 'dicendo',\n",
       " 'copiosus',\n",
       " 'ornatus',\n",
       " ';']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct['orig_tokens_all'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(unnested_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_tokens_all</th>\n",
       "      <th>agg_predictions_all</th>\n",
       "      <th>agg_labels_all</th>\n",
       "      <th>agg_ent_predictions_all</th>\n",
       "      <th>agg_ent_labels_all</th>\n",
       "      <th>all_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>CW_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iuli</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>CW_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caesaris</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>I-PERS</td>\n",
       "      <td>CW_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Commentariorum</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>De</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13971</th>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13972</th>\n",
       "      <td>dicendo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13973</th>\n",
       "      <td>copiosus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13974</th>\n",
       "      <td>ornatus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13975</th>\n",
       "      <td>;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13976 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      orig_tokens_all  agg_predictions_all  agg_labels_all  \\\n",
       "0                  C.                    1               1   \n",
       "1                Iuli                    2               2   \n",
       "2            Caesaris                    2               2   \n",
       "3      Commentariorum                    0               0   \n",
       "4                  De                    0               0   \n",
       "...               ...                  ...             ...   \n",
       "13971              in                    0               0   \n",
       "13972         dicendo                    0               0   \n",
       "13973        copiosus                    0               0   \n",
       "13974         ornatus                    0               0   \n",
       "13975               ;                    0               0   \n",
       "\n",
       "      agg_ent_predictions_all agg_ent_labels_all            all_ids  \n",
       "0                      B-PERS             B-PERS               CW_0  \n",
       "1                      I-PERS             I-PERS               CW_0  \n",
       "2                      I-PERS             I-PERS               CW_0  \n",
       "3                           O                  O               CW_0  \n",
       "4                           O                  O               CW_0  \n",
       "...                       ...                ...                ...  \n",
       "13971                       O                  O  PlinyYounger_1326  \n",
       "13972                       O                  O  PlinyYounger_1326  \n",
       "13973                       O                  O  PlinyYounger_1326  \n",
       "13974                       O                  O  PlinyYounger_1326  \n",
       "13975                       O                  O  PlinyYounger_1326  \n",
       "\n",
       "[13976 rows x 6 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-GRP       0.91      0.82      0.87       211\n",
      "       B-LOC       0.84      0.84      0.84       127\n",
      "      B-PERS       0.91      0.93      0.92       471\n",
      "       I-GRP       0.00      0.00      0.00         2\n",
      "       I-LOC       0.00      0.00      0.00         7\n",
      "      I-PERS       0.82      0.90      0.86       129\n",
      "           O       1.00      0.99      1.00     13029\n",
      "\n",
      "    accuracy                           0.99     13976\n",
      "   macro avg       0.64      0.64      0.64     13976\n",
      "weighted avg       0.99      0.99      0.99     13976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df.agg_ent_labels_all, df.agg_ent_predictions_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('errors_validation_set_herodotus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, extra, tags, wp_tokens. If id, tokens, extra, tags, wp_tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3409\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]\n",
    "    \n",
    "true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]\n",
    "\n",
    "tokens = tokenized_dataset['test']['extra']\n",
    "orig_tokens = tokenized_dataset['test']['tokens']\n",
    "ids = tokenized_dataset['test']['id']\n",
    "\n",
    "dct = {\n",
    "    'orig_tokens_all': [],\n",
    "    'agg_predictions_all': [],\n",
    "    'agg_labels_all': [],\n",
    "    'agg_ent_predictions_all': [],\n",
    "    'agg_ent_labels_all': [],\n",
    "    'all_ids': []\n",
    "}\n",
    "\n",
    "major_l = list(zip(ids, orig_tokens, tokens, true_predictions, true_labels))\n",
    "\n",
    "for idd, original_tokens, wp_tokens, preds, labels in major_l:\n",
    "    try:\n",
    "        wp_tokens = [token for token in wp_tokens if token not in ['[CLS]', '[PAD]', '[SEP]']] \n",
    "        orig_tokens, fixed_preds, fixed_labels = aggregate_ents(original_tokens, wp_tokens, preds, labels)\n",
    "        dct['orig_tokens_all'].append(orig_tokens)\n",
    "        dct['agg_predictions_all'].append(fixed_preds)\n",
    "        dct['agg_labels_all'].append(fixed_labels)\n",
    "        dct['agg_ent_predictions_all'].append([idx2label[pred] for pred in fixed_preds])\n",
    "        dct['agg_ent_labels_all'].append([idx2label[label] for label in fixed_labels])\n",
    "        dct['all_ids'].append([idd] * len(fixed_labels))\n",
    "    except AssertionError:\n",
    "        print('\\nOh no!\\n')\n",
    "        print(original_tokens)\n",
    "        print(wp_tokens)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP       0.87      0.63      0.73       349\n",
      "         LOC       0.73      0.64      0.68       280\n",
      "        PERS       0.85      0.79      0.82       859\n",
      "\n",
      "   micro avg       0.84      0.72      0.77      1488\n",
      "   macro avg       0.82      0.69      0.75      1488\n",
      "weighted avg       0.84      0.72      0.77      1488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(dct['agg_ent_labels_all'], dct['agg_ent_predictions_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnested_dct = {\n",
    "    key: sum(value, []) for key, value in dct.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31352\n",
      "31352\n",
      "31352\n",
      "31352\n",
      "31352\n",
      "31352\n"
     ]
    }
   ],
   "source": [
    "for key, value in unnested_dct.items():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3409\n",
      "3409\n",
      "3409\n",
      "3409\n",
      "3409\n",
      "3409\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Erat',\n",
       " 'in',\n",
       " 'consilio',\n",
       " 'Sertorianum',\n",
       " 'illud',\n",
       " 'exemplum',\n",
       " ',',\n",
       " 'qui',\n",
       " 'robustissimum',\n",
       " 'et',\n",
       " 'infirmissimum',\n",
       " 'militem',\n",
       " 'iussit',\n",
       " 'caudam',\n",
       " 'equi',\n",
       " '-',\n",
       " 'reliqua',\n",
       " 'nosti',\n",
       " '.']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct['orig_tokens_all'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(unnested_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_tokens_all</th>\n",
       "      <th>agg_predictions_all</th>\n",
       "      <th>agg_labels_all</th>\n",
       "      <th>agg_ent_predictions_all</th>\n",
       "      <th>agg_ent_labels_all</th>\n",
       "      <th>all_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timere</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caesarem</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>CW_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ereptis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31347</th>\n",
       "      <td>equi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31348</th>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>reliqua</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>nosti</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31352 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      orig_tokens_all  agg_predictions_all  agg_labels_all  \\\n",
       "0              timere                    0               0   \n",
       "1            Caesarem                    1               1   \n",
       "2             ereptis                    0               0   \n",
       "3                  ab                    0               0   \n",
       "4                  eo                    0               0   \n",
       "...               ...                  ...             ...   \n",
       "31347            equi                    0               0   \n",
       "31348               -                    0               0   \n",
       "31349         reliqua                    0               0   \n",
       "31350           nosti                    0               0   \n",
       "31351               .                    0               0   \n",
       "\n",
       "      agg_ent_predictions_all agg_ent_labels_all            all_ids  \n",
       "0                           O                  O              CW_11  \n",
       "1                      B-PERS             B-PERS              CW_11  \n",
       "2                           O                  O              CW_11  \n",
       "3                           O                  O              CW_11  \n",
       "4                           O                  O              CW_11  \n",
       "...                       ...                ...                ...  \n",
       "31347                       O                  O  PlinyYounger_1334  \n",
       "31348                       O                  O  PlinyYounger_1334  \n",
       "31349                       O                  O  PlinyYounger_1334  \n",
       "31350                       O                  O  PlinyYounger_1334  \n",
       "31351                       O                  O  PlinyYounger_1334  \n",
       "\n",
       "[31352 rows x 6 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-GRP       0.89      0.64      0.74       349\n",
      "       B-LOC       0.77      0.66      0.71       280\n",
      "      B-PERS       0.89      0.80      0.84       857\n",
      "       I-GRP       0.00      0.00      0.00         3\n",
      "       I-LOC       0.00      0.00      0.00         9\n",
      "      I-PERS       0.77      0.88      0.82       104\n",
      "           O       0.99      1.00      0.99     29750\n",
      "\n",
      "    accuracy                           0.98     31352\n",
      "   macro avg       0.61      0.57      0.59     31352\n",
      "weighted avg       0.98      0.98      0.98     31352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pricie/marijkeb/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df.agg_ent_labels_all, df.agg_ent_predictions_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['all_ids'].apply(lambda x: x.split('_')[0])\n",
    "df['sentence'] = df['all_ids'].apply(lambda x: x.split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ovid            16718\n",
       "GW               7663\n",
       "PlinyElder       3927\n",
       "PlinyYounger     2479\n",
       "CW                565\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['domain'] = df['text'].apply(lambda x: 'IN' if x != 'Ovid' else 'OUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'orig_tokens_all': 'token',\n",
    "                    'agg_ent_predictions_all': 'predictions',\n",
    "                    'agg_ent_labels_all': 'labels',\n",
    "                    'all_ids': 'sentence_ids',\n",
    "                    'agg_predictions_all': 'prediction_id',\n",
    "                    'agg_labels_all': 'label_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['token', 'label_id', 'prediction_id', 'labels', 'predictions', 'sentence_ids', 'text', 'sentence', 'domain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label_id</th>\n",
       "      <th>prediction_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>predictions</th>\n",
       "      <th>sentence_ids</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timere</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "      <td>CW</td>\n",
       "      <td>11</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caesarem</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>B-PERS</td>\n",
       "      <td>CW_11</td>\n",
       "      <td>CW</td>\n",
       "      <td>11</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ereptis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "      <td>CW</td>\n",
       "      <td>11</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "      <td>CW</td>\n",
       "      <td>11</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>CW_11</td>\n",
       "      <td>CW</td>\n",
       "      <td>11</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31347</th>\n",
       "      <td>equi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "      <td>PlinyYounger</td>\n",
       "      <td>1334</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31348</th>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "      <td>PlinyYounger</td>\n",
       "      <td>1334</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>reliqua</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "      <td>PlinyYounger</td>\n",
       "      <td>1334</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>nosti</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "      <td>PlinyYounger</td>\n",
       "      <td>1334</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PlinyYounger_1334</td>\n",
       "      <td>PlinyYounger</td>\n",
       "      <td>1334</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31352 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  label_id  prediction_id  labels predictions  \\\n",
       "0        timere         0              0       O           O   \n",
       "1      Caesarem         1              1  B-PERS      B-PERS   \n",
       "2       ereptis         0              0       O           O   \n",
       "3            ab         0              0       O           O   \n",
       "4            eo         0              0       O           O   \n",
       "...         ...       ...            ...     ...         ...   \n",
       "31347      equi         0              0       O           O   \n",
       "31348         -         0              0       O           O   \n",
       "31349   reliqua         0              0       O           O   \n",
       "31350     nosti         0              0       O           O   \n",
       "31351         .         0              0       O           O   \n",
       "\n",
       "            sentence_ids          text sentence domain  \n",
       "0                  CW_11            CW       11     IN  \n",
       "1                  CW_11            CW       11     IN  \n",
       "2                  CW_11            CW       11     IN  \n",
       "3                  CW_11            CW       11     IN  \n",
       "4                  CW_11            CW       11     IN  \n",
       "...                  ...           ...      ...    ...  \n",
       "31347  PlinyYounger_1334  PlinyYounger     1334     IN  \n",
       "31348  PlinyYounger_1334  PlinyYounger     1334     IN  \n",
       "31349  PlinyYounger_1334  PlinyYounger     1334     IN  \n",
       "31350  PlinyYounger_1334  PlinyYounger     1334     IN  \n",
       "31351  PlinyYounger_1334  PlinyYounger     1334     IN  \n",
       "\n",
       "[31352 rows x 9 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('errors_test_set_herodotos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for match with csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('./data/DBG_eval.csv', index_col=0)\n",
    "\n",
    "#the following is necessary because the eval.csv is not in ascending sentence order and the json is\n",
    "grouped = val.groupby('sentence')\n",
    "\n",
    "val2 = pd.DataFrame(columns = ['word', 'tag', 'sentence'])\n",
    "\n",
    "for name, group in grouped:\n",
    "    val2 = pd.concat([val2, group])\n",
    "\n",
    "\n",
    "assert val2['tag'].values.tolist() == unnested_dct['agg_ent_labels_all']\n",
    "assert val2['word'].values.tolist() == unnested_dct['orig_tokens_all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaghetti mess starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file trained_lat_BERT/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"trained_lat_BERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PERS\",\n",
      "    \"2\": \"I-PERS\",\n",
      "    \"3\": \"B-LOC\",\n",
      "    \"4\": \"I-LOC\",\n",
      "    \"5\": \"B-GRP\",\n",
      "    \"6\": \"I-GRP\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"start_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32900\n",
      "}\n",
      "\n",
      "loading weights file trained_lat_BERT/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at trained_lat_BERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('trained_lat_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "from transformers.pipelines import TokenClassificationPipeline\n",
    "\n",
    "def softmax(outputs):\n",
    "    maxes = np.max(outputs, axis=-1, keepdims=True)\n",
    "    shifted_exp = np.exp(outputs - maxes)\n",
    "    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n",
    "\n",
    "class LatinNerPipeline(TokenClassificationPipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"split_on_words\" in kwargs:\n",
    "            preprocess_kwargs[\"split_on_words\"] = kwargs[\"split_on_words\"]\n",
    "        if \"tokenizer\" in kwargs:\n",
    "            preprocess_kwargs[\"tokenizer\"] = kwargs[\"tokenizer\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, split_on_words=False, tokenizer=tokenizer):\n",
    "        model_inputs = {}\n",
    "        if split_on_words:\n",
    "            tokens = text\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "\n",
    "        wp_tokens = tokenizer.tokenize(tokens) #create wordpiece tokens\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(wp_tokens) #to ids\n",
    "        model_inputs['input_ids'] = [token_ids]\n",
    "        model_inputs['attention_mask'] = [tokenizer.calculate_attention_masks(token_ids)]\n",
    "        model_inputs['wp_tokens'] = [wp_tokens]       \n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        logits = outputs.logits[0].detach().numpy()\n",
    "        \n",
    "        probabilities = [softmax(i) for i in logits]\n",
    "\n",
    "        best_classes = [np.argmax(prob) for prob in probabilities]\n",
    "        labels = [self.model.config.id2label[best_class] for best_class in best_classes]\n",
    "        scores = [probabilities[best_class].item() for best_class in best_classes]\n",
    "        logits = logits.tolist()\n",
    "        return {\"labels\": label, \"scores\": score, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LatinNerPipeline at 0x7f517b2e2b60>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sents = [val.groupby('sentence').get_group(group)['word'].tolist() for group in val.groupby('sentence').groups]\n",
    "\n",
    "LatinNerPipeline(inputs = val_sents[0], model=model, split_on_words=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gallos', 'ab', 'Aquitanis', 'Garumna', 'flumen', ',', 'a', 'Belgis', 'Matrona', 'et', 'Sequana', 'dividit', '.']\n",
      "torch.Size([1, 20])\n",
      "[0.04528868 0.05210729 0.00390455 0.6026168  0.02704006 0.25807783\n",
      " 0.0109648 ]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import TokenClassificationPipeline, AutoTokenizer\n",
    "def softmax(outputs):\n",
    "    maxes = np.max(outputs, axis=-1, keepdims=True)\n",
    "    shifted_exp = np.exp(outputs - maxes)\n",
    "    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n",
    "\n",
    "val_sents = [val.groupby('sentence').get_group(group)['word'].tolist() for group in val.groupby('sentence').groups]\n",
    "\n",
    "print(val_sents[0])\n",
    "\n",
    "def tokenize(text, split_on_words=True):\n",
    "    output = {}\n",
    "    if split_on_words:\n",
    "        tokens = text\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "\n",
    "    wp_tokens = tokenizer.tokenize(tokens) #create wordpiece tokens\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(wp_tokens) #to ids\n",
    "    output['input_ids'] = [token_ids]\n",
    "    output['attention_mask'] = [tokenizer.calculate_attention_masks(token_ids)]\n",
    "    output = BatchEncoding(output, tensor_type=\"pt\")\n",
    "    print(output['input_ids'].shape)\n",
    "    \n",
    "    return wp_tokens, output\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "wp_tokens, pt = tokenize(val_sents[0])\n",
    "\n",
    "outputs = model(**pt)\n",
    "\n",
    "logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "# test = m(logits)\n",
    "\n",
    "logits_np = outputs.logits[0].detach().numpy()\n",
    "\n",
    "# print(np.sum(softmax(logits_np[0])))\n",
    "print(softmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[-1.5679e-02,  1.2457e-01, -2.4666e+00,  2.5725e+00, -5.3142e-01,\n",
      "           1.7245e+00, -1.4340e+00],\n",
      "         [ 7.0129e+00, -1.7037e+00, -1.0604e+00, -1.3850e+00, -1.7884e+00,\n",
      "           1.0293e-01, -2.0452e+00],\n",
      "         [-1.8242e-01, -5.4679e-01, -1.0210e+00, -2.7274e-01, -8.5632e-01,\n",
      "           6.8070e+00, -7.4175e-01],\n",
      "         [-1.7124e-01, -5.4383e-01, -1.1186e+00, -4.1450e-01, -8.3852e-01,\n",
      "           6.7640e+00, -8.8571e-01],\n",
      "         [-6.4413e-02, -3.3789e-01, -1.1795e+00, -3.8483e-01, -7.6377e-01,\n",
      "           6.5760e+00, -9.9100e-01],\n",
      "         [-2.1962e+00,  3.7701e-01, -1.4618e+00,  5.3211e+00, -2.3715e-01,\n",
      "          -1.3182e-01, -2.0692e+00],\n",
      "         [-2.0182e+00,  6.3216e-01, -1.5570e+00,  5.2229e+00, -3.6870e-01,\n",
      "          -6.2343e-01, -2.2126e+00],\n",
      "         [-1.8010e+00,  7.4037e-01, -1.2488e+00,  4.8491e+00, -2.9059e-01,\n",
      "          -5.9628e-01, -2.0196e+00],\n",
      "         [ 6.8726e+00, -1.2977e+00, -1.4145e+00, -8.1941e-01, -2.0182e+00,\n",
      "          -4.4086e-01, -2.1465e+00],\n",
      "         [ 7.2874e+00, -1.0884e+00, -1.5284e+00, -9.2879e-01, -2.1368e+00,\n",
      "          -4.8484e-01, -1.9547e+00],\n",
      "         [ 6.5584e+00, -1.7830e+00, -7.5086e-01, -1.2856e+00, -1.8154e+00,\n",
      "           4.1349e-01, -2.0227e+00],\n",
      "         [-3.1817e-02, -6.4275e-01, -1.1493e+00, -5.0251e-01, -7.2406e-01,\n",
      "           6.7252e+00, -7.7667e-01],\n",
      "         [-3.3267e-02, -3.7057e-01, -1.2571e+00, -4.5926e-01, -6.3228e-01,\n",
      "           6.5490e+00, -8.8825e-01],\n",
      "         [-1.2937e+00,  4.9228e-03, -1.5010e+00,  5.4097e+00, -3.6729e-01,\n",
      "          -6.3508e-01, -2.1456e+00],\n",
      "         [-1.1956e+00,  3.1001e-01, -1.2699e+00,  4.9896e+00, -4.9475e-01,\n",
      "          -4.4303e-01, -2.2147e+00],\n",
      "         [ 6.6087e+00, -1.0455e+00, -1.5002e+00, -5.2499e-01, -1.9444e+00,\n",
      "          -4.5597e-01, -2.4186e+00],\n",
      "         [-1.6711e+00,  2.5050e-01, -1.3705e+00,  5.2225e+00, -1.6569e-01,\n",
      "          -2.3620e-01, -2.1042e+00],\n",
      "         [-1.7084e+00,  5.9807e-01, -1.4249e+00,  5.2610e+00, -4.2801e-01,\n",
      "          -3.7162e-01, -2.2275e+00],\n",
      "         [ 7.3903e+00, -1.4639e+00, -1.6355e+00, -1.5157e+00, -1.8728e+00,\n",
      "          -2.0771e-01, -2.0083e+00],\n",
      "         [ 7.5419e+00, -1.2039e+00, -1.7907e+00, -1.2875e+00, -1.9653e+00,\n",
      "          -4.2313e-01, -2.0253e+00]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[[1.1585e-04, 6.6736e-02, 1.6101e-02, 1.0193e-02, 6.5494e-02,\n",
      "          1.3900e-03, 5.8571e-02],\n",
      "         [1.3073e-01, 1.0724e-02, 6.5699e-02, 1.9479e-04, 1.8633e-02,\n",
      "          2.7464e-04, 3.1788e-02],\n",
      "         [9.8059e-05, 3.4103e-02, 6.8339e-02, 5.9239e-04, 4.7326e-02,\n",
      "          2.2404e-01, 1.1704e-01],\n",
      "         [9.9162e-05, 3.4204e-02, 6.1986e-02, 5.1409e-04, 4.8175e-02,\n",
      "          2.1459e-01, 1.0135e-01],\n",
      "         [1.1034e-04, 4.2026e-02, 5.8319e-02, 5.2958e-04, 5.1914e-02,\n",
      "          1.7782e-01, 9.1222e-02],\n",
      "         [1.3090e-05, 8.5900e-02, 4.3977e-02, 1.5921e-01, 8.7902e-02,\n",
      "          2.1718e-04, 3.1035e-02],\n",
      "         [1.5640e-05, 1.1087e-01, 3.9983e-02, 1.4432e-01, 7.7067e-02,\n",
      "          1.3284e-04, 2.6887e-02],\n",
      "         [1.9434e-05, 1.2354e-01, 5.4418e-02, 9.9308e-02, 8.3327e-02,\n",
      "          1.3649e-04, 3.2614e-02],\n",
      "         [1.1362e-01, 1.6095e-02, 4.6109e-02, 3.4292e-04, 1.4808e-02,\n",
      "          1.5944e-04, 2.8727e-02],\n",
      "         [1.7203e-01, 1.9842e-02, 4.1143e-02, 3.0739e-04, 1.3152e-02,\n",
      "          1.5258e-04, 3.4798e-02],\n",
      "         [8.2987e-02, 9.9062e-03, 8.9532e-02, 2.1515e-04, 1.8138e-02,\n",
      "          3.7466e-04, 3.2511e-02],\n",
      "         [1.1400e-04, 3.0982e-02, 6.0110e-02, 4.7078e-04, 5.4018e-02,\n",
      "          2.0644e-01, 1.1303e-01],\n",
      "         [1.1383e-04, 4.0675e-02, 5.3966e-02, 4.9159e-04, 5.9210e-02,\n",
      "          1.7309e-01, 1.0109e-01],\n",
      "         [3.2274e-05, 5.9210e-02, 4.2286e-02, 1.7397e-01, 7.7175e-02,\n",
      "          1.3130e-04, 2.8752e-02],\n",
      "         [3.5603e-05, 8.0333e-02, 5.3280e-02, 1.1430e-01, 6.7940e-02,\n",
      "          1.5910e-04, 2.6832e-02],\n",
      "         [8.7260e-02, 2.0712e-02, 4.2322e-02, 4.6032e-04, 1.5942e-02,\n",
      "          1.5705e-04, 2.1883e-02],\n",
      "         [2.2129e-05, 7.5692e-02, 4.8183e-02, 1.4426e-01, 9.4413e-02,\n",
      "          1.9565e-04, 2.9966e-02],\n",
      "         [2.1318e-05, 1.0715e-01, 4.5630e-02, 1.4994e-01, 7.2628e-02,\n",
      "          1.7087e-04, 2.6492e-02],\n",
      "         [1.9067e-01, 1.3629e-02, 3.6966e-02, 1.7092e-04, 1.7125e-02,\n",
      "          2.0131e-04, 3.2982e-02],\n",
      "         [2.2188e-01, 1.7678e-02, 3.1652e-02, 2.1473e-04, 1.5613e-02,\n",
      "          1.6229e-04, 3.2428e-02]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "outputs = model(**pt)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "\n",
    "predictions = m(outputs.logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds[0]))\n",
    "print(len(predictions.label_ids[0]))\n",
    "print(predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = [\n",
    "        [idx2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]\n",
    "    \n",
    "true_labels = [\n",
    "        [idx2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, predictions.label_ids)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report([label for lst in true_labels for label in lst], [pred for lst in true_predictions for pred in lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels,true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([label for lst in true_labels for label in lst])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
